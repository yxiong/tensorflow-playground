{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "import flowers_dataset\n",
    "import inception_preprocessing\n",
    "import inception_v1\n",
    "import model_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deployment flags.\n",
    "task_id = 0                # Task id of the replica running the training.\n",
    "worker_replicas = 1        # Number of worker replicas.\n",
    "\n",
    "# Dataset flags.\n",
    "dataset_split_name = \"train\"\n",
    "home_dir = os.path.expanduser('~')\n",
    "base_data_dir = os.path.join(home_dir, \"data/flowers\")\n",
    "dataset_dir = os.path.join(base_data_dir, \"tfrecords\")\n",
    "train_dir = os.path.join(base_data_dir, \"train_logs/v003\")\n",
    "\n",
    "# Training flags.\n",
    "batch_size = 32\n",
    "num_preprocessing_threads = 4\n",
    "save_summaries_secs = 600\n",
    "save_interval_secs = 600\n",
    "log_every_n_steps = 10\n",
    "\n",
    "# Used for pre-training.\n",
    "checkpoint_path = None\n",
    "checkpoint_exclude_scopes = None\n",
    "ignore_missing_vars = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def configure_learning_rate(num_samples_per_epoch, global_step):\n",
    "    initial_learning_rate = 0.01\n",
    "    num_epochs_per_decay = 2\n",
    "    learning_rate_decay_factor = 0.94\n",
    "    decay_steps = int(num_samples_per_epoch / batch_size * num_epochs_per_decay)\n",
    "    return tf.train.exponential_decay(initial_learning_rate,\n",
    "                                      global_step,\n",
    "                                      decay_steps,\n",
    "                                      learning_rate_decay_factor,\n",
    "                                      staircase=True,\n",
    "                                      name='exponential_decay_learning_rate')\n",
    "\n",
    "\n",
    "def configure_optimizer(learning_rate):\n",
    "    optimizer_type = \"rmsprop\"\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate,\n",
    "                beta1=FLAGS.adam_beta1,\n",
    "                beta2=FLAGS.adam_beta2,\n",
    "                epsilon=FLAGS.opt_epsilon)\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "                learning_rate,\n",
    "                momentum=0.9,\n",
    "                epsilon=1.0)\n",
    "    else:\n",
    "        raise ValueError('Optimizer [%s] was not recognized', optimizer_type)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_init_fn(checkpoint_path, checkpoint_exclude_scopes, ignore_missing_vars):\n",
    "    if checkpoint_path is None:\n",
    "        return None\n",
    "\n",
    "    exclusions = []\n",
    "    if checkpoint_exclude_scopes:\n",
    "        exclusions = [scope.strip() for scope in checkpoint_exclude_scopes.split(',')]\n",
    "\n",
    "    variables_to_restore = []\n",
    "    for var in slim.get_model_variables():\n",
    "        excluded = False\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                excluded = True\n",
    "                break\n",
    "        if not excluded:\n",
    "            variables_to_restore.append(var)\n",
    "\n",
    "    if tf.gfile.IsDirectory(checkpoint_path):\n",
    "        checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint_path = checkpoint_path\n",
    "\n",
    "    tf.logging.info('Fine-tuning from %s' % checkpoint_path)\n",
    "\n",
    "    return slim.assign_from_checkpoint_fn(\n",
    "            checkpoint_path,\n",
    "            variables_to_restore,\n",
    "            ignore_missing_vars=ignore_missing_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "with tf.Graph().as_default():\n",
    "    # Config model_deploy\n",
    "    deploy_config = model_deploy.DeploymentConfig(\n",
    "        num_clones=1,\n",
    "        clone_on_cpu=False,\n",
    "        replica_id=task_id,\n",
    "        num_replicas=worker_replicas,\n",
    "        num_ps_tasks=0)\n",
    "\n",
    "    # Create global_step\n",
    "    with tf.device(deploy_config.variables_device()):\n",
    "        global_step = slim.create_global_step()\n",
    "\n",
    "    # Select the dataset\n",
    "    dataset = flowers_dataset.get_dataset(dataset_split_name, dataset_dir)\n",
    "\n",
    "    # Create a dataset provider that loads data from the dataset\n",
    "    with tf.device(deploy_config.inputs_device()):\n",
    "        provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset,\n",
    "            num_readers=4,\n",
    "            common_queue_capacity=20 * batch_size,\n",
    "            common_queue_min=10 * batch_size)\n",
    "        [image, label] = provider.get(['image', 'label'])\n",
    "\n",
    "        # Preprocess the images.\n",
    "        train_image_size = inception_v1.inception_v1.default_image_size\n",
    "        image = inception_preprocessing.preprocess_image(image, train_image_size, train_image_size, is_training=True)\n",
    "\n",
    "        # Create training batch.\n",
    "        images, labels = tf.train.batch(\n",
    "            [image, label],\n",
    "            batch_size=batch_size,\n",
    "            num_threads=num_preprocessing_threads,\n",
    "            capacity=5 * batch_size)\n",
    "        labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "        batch_queue = slim.prefetch_queue.prefetch_queue([images, labels], capacity=2 * deploy_config.num_clones)\n",
    "\n",
    "    # Define the model\n",
    "    def clone_fn(batch_queue):\n",
    "        \"\"\"Allows data parallelism by creating multiple clones of network_fn.\"\"\"\n",
    "        images, labels = batch_queue.dequeue()\n",
    "        \n",
    "        with slim.arg_scope(inception_v1.inception_v1_arg_scope(weight_decay=0.00004)):\n",
    "            logits, end_points = inception_v1.inception_v1(\n",
    "                images, num_classes=dataset.num_classes, is_training=True)\n",
    "\n",
    "        slim.losses.softmax_cross_entropy(logits, labels, label_smoothing=0.0, weight=1.0)\n",
    "        return end_points\n",
    "\n",
    "    # Gather initial summaries.\n",
    "    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n",
    "    first_clone_scope = deploy_config.clone_scope(0)\n",
    "    # Gather update_ops from the first clone. These contain, for example,\n",
    "    # the updates for the batch_norm variables created by network_fn.\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n",
    "\n",
    "    # Add summaries for end_points.\n",
    "    end_points = clones[0].outputs\n",
    "    for end_point in end_points:\n",
    "        x = end_points[end_point]\n",
    "        summaries.add(tf.histogram_summary('activations/' + end_point, x))\n",
    "        summaries.add(tf.scalar_summary('sparsity/' + end_point,\n",
    "                                                                        tf.nn.zero_fraction(x)))\n",
    "\n",
    "    # Add summaries for losses.\n",
    "    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n",
    "        summaries.add(tf.scalar_summary('losses/%s' % loss.op.name, loss))\n",
    "\n",
    "    # Add summaries for variables.\n",
    "    for variable in slim.get_model_variables():\n",
    "        summaries.add(tf.histogram_summary(variable.op.name, variable))\n",
    "\n",
    "    # Configure the optimization procedure.\n",
    "    with tf.device(deploy_config.optimizer_device()):\n",
    "        learning_rate = configure_learning_rate(dataset.num_samples, global_step)\n",
    "        optimizer = configure_optimizer(learning_rate)\n",
    "        summaries.add(tf.scalar_summary('learning_rate', learning_rate, name='learning_rate'))\n",
    "\n",
    "    # Variables to train.\n",
    "    variables_to_train = tf.trainable_variables()\n",
    "\n",
    "    #    and returns a train_tensor and summary_op\n",
    "    total_loss, clones_gradients = model_deploy.optimize_clones(clones, optimizer, var_list=variables_to_train)\n",
    "    # Add total_loss to summary.\n",
    "    summaries.add(tf.scalar_summary('total_loss', total_loss, name='total_loss'))\n",
    "\n",
    "    # Create gradient updates.\n",
    "    grad_updates = optimizer.apply_gradients(clones_gradients, global_step=global_step)\n",
    "    update_ops.append(grad_updates)\n",
    "\n",
    "    update_op = tf.group(*update_ops)\n",
    "    train_tensor = control_flow_ops.with_dependencies([update_op], total_loss, name='train_op')\n",
    "\n",
    "    # Add the summaries from the first clone. These contain the summaries\n",
    "    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n",
    "    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES, first_clone_scope))\n",
    "\n",
    "    # Merge all summaries together.\n",
    "    summary_op = tf.merge_summary(list(summaries), name='summary_op')\n",
    "\n",
    "    # Kicks off the training.\n",
    "    slim.learning.train(\n",
    "        train_tensor,\n",
    "        logdir=train_dir,\n",
    "        master=\"\",\n",
    "        is_chief=(task_id == 0),\n",
    "        init_fn=get_init_fn(checkpoint_path, checkpoint_exclude_scopes, ignore_missing_vars),\n",
    "        summary_op=summary_op,\n",
    "        log_every_n_steps=log_every_n_steps,\n",
    "        save_summaries_secs=save_summaries_secs,\n",
    "        save_interval_secs=save_interval_secs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
